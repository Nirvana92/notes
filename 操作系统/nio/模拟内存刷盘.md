修改linux 的dirty 的配置项[脏页刷新配置]: 

```shell
# 查看dirty 配置项
> sysctl -a | grep dirty
```

**strace 追踪java进程查看系统调用。**

缓存: 程序中的buffer 缓存, cpu 中的pagecache 缓存, 磁盘中的缓存。

模拟代码1: 

开启一个fileio, 死循环一直往file中写数据。

开启一个buffer file output 一直往file中写数据。

```shell
> lsof -p <pid>
```



**BIO的弊端:** accept 系统调用[阻塞], new Thread 系统调用 阻塞[BLOCKING]; 可以池化优化一些。不过线程过多会导致频繁的上下文切换。

在SocketNIO 代码中通过设置`ServerSocketChannel.setConfigureBlocking(false);` 设置非阻塞。



**NIO:** 设置nonblocking. accept 不阻塞。`client.configureBlocking(false);` 是为了读取数据的时候不阻塞。

NIO 连接相对BIO速度已经快了很多, 但是连接相对来说还是比较慢。原因是连接的客户端多了。然后在读取数据的时候遍历clients 去读取数据会遍历很多次进行多次的系统调用读取数据。所以导致很慢。参考 SocketNIO.java 代码。



**多路复用器:** 一次调用询问内核有哪些链接可以读取数据[IO状态]了。避免了轮询一次一次的去问内核有哪些链接可以读取数据。然后由程序自己对有状态的IO进行读写操作。

多路复用器: select, poll, epoll

多路复用器标准: posix

unix: kqueue（类linux中的epoll）

只关注IO: 不关注IO读写之后的事情

同步: app自己读写

异步: 内核完成读写[写到内存的一个区域中], 好像app没有访问IO, buffer. win: iocp

阻塞: blocking: 调用方法是否阻塞

非阻塞: nonblocking: 调用方法不阻塞



linux: netty: 

同步阻塞: 同步: 程序自己读取 ，阻塞: 调用了方法一直等待有效返回结果。

同步非阻塞: 程序自己读取，调用方法一瞬间，给出是否读到[自己要解决下一次什么时候去读取]

异步: 尽量不要讨论, 因为现在只讨论IO模型下, linux目前没有通用内核的异步处理方案。

异步: 阻塞? 异步只有非阻塞, 阻塞没有意义。



**多路复用器-select:** synchronous I/O multiplexing;  同步的io多路复用器。FD_SETSIZE[1024] 多路的限制。

select(fds): 传入的所有文件描述符获取io状态。

**多路复用器-poll:** poll没有 FD_SETSIZE（1024）的限制。

select、poll、nio 其实都是要遍历询问所有io的状态。只不过select和poll将fds给到内核去遍历了。NIO 这个遍历的过程在用户态内核态切换。select,poll: 这个遍历的过程触发了一次系统调用，用户态内核态的切换，过程中把fds传递给内核，内核重新根据用户这次调用传过来的fds遍历修改状态。

select、poll的弊端: 

1. 每次都要重新重复传递fds。
2. 每次内核被调用了之后，针对这次调用，触发一次遍历fds全量的复杂度。

**多路复用器-epoll:** 规避了遍历。网卡接受数据通过事件直接放到内存区域中。后续调用epoll_wait() 的时候直接拿取相关数据。

低啊用过程: socket-fd4 -> bind -> listen fd4 -> epoll_create  -> epoll_ctl -> epoll_wait()